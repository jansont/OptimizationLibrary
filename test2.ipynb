{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, eig\n",
    "from functools import partial\n",
    "from algorithms import conjugate_gradient, secant, Finite_Difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step_size(p, x, s, gamma=1.5, mu=0.8):\n",
    "    \"\"\"Armijo algorithm for computing step size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float\n",
    "        Parameter for increasing step size\n",
    "    mu : float\n",
    "        Parameter for decreasing step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Step size\n",
    "    \"\"\"\n",
    "\n",
    "    w = 1  # Default step size\n",
    "\n",
    "    k_g = 0  # Power of gamma\n",
    "    k_m = 0  # Power of mu\n",
    "\n",
    "    # Precompute cost and gradient to save time\n",
    "    vx = p.cost(x)\n",
    "    gx_s = p.grad(x) @ s\n",
    "\n",
    "    def v_bar(w):\n",
    "        return vx + 0.5 * w * gx_s\n",
    "\n",
    "    while p.cost(x + gamma**k_g * s) < v_bar(gamma**k_g):\n",
    "        k_g += 1\n",
    "        w = gamma**k_g\n",
    "\n",
    "    while p.cost(x + mu**k_m * gamma**k_g * s) > v_bar(mu**k_m * gamma**k_g):\n",
    "        k_m += 1\n",
    "        w = mu**k_m * gamma**k_g\n",
    "\n",
    "    return w\n",
    "def _fd_grad(f, x, h=1e-8):\n",
    "    \"\"\"Finite difference approximation of the gradient\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function of x (2D numpy column array)\n",
    "        Function whose gradient you want to evaluate\n",
    "    x : 2D numpy column array\n",
    "        Point where you want to evaluate the gradient\n",
    "    h : float\n",
    "        Step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    2D numpy row array\n",
    "        Gradient (which is a row vector)\n",
    "    \"\"\"\n",
    "\n",
    "    dim = np.max(np.shape(x))\n",
    "    grad_gen = ((f(x + h * np.eye(dim)[:, [i]]) - f(x)) / h\n",
    "               for i in range(0, dim))\n",
    "    grad = np.expand_dims(np.fromiter(grad_gen, np.float64), axis=0)\n",
    "    return grad\n",
    "\n",
    "class Problem:\n",
    "    \"\"\"Optimization problem\"\"\"\n",
    "\n",
    "    def __init__(self, cost, grad=None, grad_step=None,\n",
    "                 eq_const=None, ineq_const=None):\n",
    "        \"\"\"Constructor for Problem object\n",
    "\n",
    "        Creates a Problem object for use with optimization algorithms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cost : function whose input is 2D numpy column array\n",
    "            Objective function of problem\n",
    "        grad : function whose input is 2D numpy column array\n",
    "            Gradient function of problem. If not specified, finite difference\n",
    "            is used\n",
    "        grad_step : float\n",
    "            Step size for finite difference gradient. If not specified, (and\n",
    "            no analytic gradient is given) 1e-8 is used\n",
    "        eq_const : list of functions\n",
    "            List of functions that form equality constraints for problem\n",
    "        ineq_const : list of functions\n",
    "            List of functions that form inequality constraints for problem\n",
    "\n",
    "        \"\"\"\n",
    "        self._cost = cost\n",
    "        # Check presence of gradient function\n",
    "        if grad is None and grad_step is None:\n",
    "            # No gradient function, default step size\n",
    "            self._grad_step = 1e-8\n",
    "            self._grad = partial(_fd_grad, self.cost, h=self._grad_step)\n",
    "        elif grad is None and grad_step is not None:\n",
    "            # No gradient function, specified step size\n",
    "            self._grad_step = grad_step\n",
    "            self._grad = partial(_fd_grad, self.cost, h=self._grad_step)\n",
    "        elif grad is not None:\n",
    "            # Gradient given, no need for step size\n",
    "            self._grad_step = None\n",
    "            self._grad = grad\n",
    "        # Check presence of constraints\n",
    "        if eq_const is not None:\n",
    "            self._eq_const = eq_const\n",
    "        else:\n",
    "            self._eq_const = None\n",
    "        if ineq_const is not None:\n",
    "            self._ineq_const = ineq_const\n",
    "        else:\n",
    "            self._ineq_const = None\n",
    "\n",
    "    def cost(self, x=None):\n",
    "        \"\"\"Cost of Problem\n",
    "\n",
    "        If x is given, returns cost at x. Otherwise returns cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 2D numpy column array\n",
    "            Point at which to evaluate cost\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or function\n",
    "            If x was given, returns cost at x. Otherwise returns cost function.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            return self._cost(x)\n",
    "        else:\n",
    "            return self._cost\n",
    "\n",
    "    def grad(self, x=None):\n",
    "        \"\"\"Gradient of Problem\n",
    "\n",
    "        If x is given, returns gradient at x. Otherwise returns gradient\n",
    "        function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 2D numpy column array\n",
    "            Point at which to evaluate gradient\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        2D numpy row array or function\n",
    "            If x was given, returns gradient at x. Otherwise returns gradient\n",
    "            function.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            return self._grad(x)\n",
    "        else:\n",
    "            return self._grad\n",
    "\n",
    "    def eq_const(self, x=None):\n",
    "        \"\"\" Equality constraints of Problem\n",
    "\n",
    "        If x is given, returns column array of constraints evaluated\n",
    "        at x. Otherwise returns column array of functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 2D numpy column array\n",
    "            Point at which to evaluate constraints\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        2D numpy column array of floats or functions\n",
    "            If x was given, returns column array of costs at x. Otherwise\n",
    "            returns column array of functions\n",
    "        \"\"\"\n",
    "        if self._eq_const is not None:\n",
    "            if x is not None:\n",
    "                return np.array([[eq(x)] for eq in self._eq_const])\n",
    "            else:\n",
    "                return np.array([eq for eq in self._eq_const])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def ineq_const(self, x=None):\n",
    "        \"\"\" Inequality constraints of Problem\n",
    "\n",
    "        If x is given, returns column array of constraints evaluated\n",
    "        at x. Otherwise returns column array of functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 2D numpy column array\n",
    "            Point at which to evaluate constraints\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        2D numpy column array of floats or functions\n",
    "            If x was given, returns column array of costs at x. Otherwise\n",
    "            returns column array of functions\n",
    "        \"\"\"\n",
    "        if self._ineq_const is not None:\n",
    "            if x is not None:\n",
    "                return np.array([[ineq(x)] for ineq in self._ineq_const])\n",
    "            else:\n",
    "                return np.array([ineq for ineq in self._ineq_const])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def num_eq_const(self):\n",
    "        \"\"\"Returns number of equality constraints\"\"\"\n",
    "        if self._eq_const is not None:\n",
    "            return np.max(np.shape(self._eq_const))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def num_ineq_const(self):\n",
    "        \"\"\"Returns number of inequality constraints\"\"\"\n",
    "        if self._ineq_const is not None:\n",
    "            return np.max(np.shape(self._ineq_const))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "def steepest_descent(p, x, tol=1e-6, max_iter=999, hist=False):\n",
    "    \"\"\"Steepest descent optimization algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Problem\n",
    "        Problem to minimize\n",
    "    x : 2D numpy column array of floats\n",
    "        Initial guess at minimum\n",
    "    tol : float\n",
    "        When norm of gradient goes below this value, iteration stops\n",
    "    max_iter : int\n",
    "        Absolute maximum number of iterations before giving up and returning x\n",
    "    hist : bool\n",
    "        If True, returns array with value of x at every iteration. If False,\n",
    "        just returns last x value.o\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    2D or 3D numpy column array of floats\n",
    "        If hist is False, returns 2D numpy colmn array containing minimizing\n",
    "        x of problem. Otherwise returns a 3D numpy array containing every\n",
    "        value of x along the way.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    x_hist = []\n",
    "    while np.linalg.norm(p.grad(x)) > tol:\n",
    "        if i > max_iter:\n",
    "            break\n",
    "        s = -p.grad(x).T\n",
    "        t0 = time.time()\n",
    "        w = _step_size(p, x, s)\n",
    "       \n",
    "        x_hist.append(x)\n",
    "        x = x + w * s\n",
    "        i += 1\n",
    "\n",
    "    return x if not hist else np.array(x_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fd_hessian(f, x, h=1e-8):\n",
    "    \"\"\"Finite different approximation of the Hessian\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function of x (2D numpy column array)\n",
    "        Function whose Hessian you want to evaluate\n",
    "    x : 2D numpy column array\n",
    "        Point where you want to evaluate the Hessian\n",
    "    h : float\n",
    "        Step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    2D numpy matrixx\n",
    "        Hessian matrix\n",
    "    \"\"\"\n",
    "\n",
    "    dim = np.max(np.shape(x))\n",
    "    I = np.eye(dim)\n",
    "    H = np.zeros((dim, dim))\n",
    "\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, dim):\n",
    "            H[i, j] =  (f(x + h * I[:, [i]] + h * I[:, [j]]) \\\n",
    "                      - f(x + h * I[:, [i]]) - f(x + h * I[:, [j]]) \\\n",
    "                      + f(x)) / h**2\n",
    "\n",
    "    return 0.5 * (H + H.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_newton(p, x0, tol=1e-6, hist=False):\n",
    "    \"\"\"Constrained optimization algorithm using Lagrange-Newton method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Problem\n",
    "        Problem to minimize (needs constraints)\n",
    "    x0 : 2D numpy column array of floats\n",
    "        Initial guess at minimum\n",
    "    tol : float\n",
    "        When norm of gradient goes below this value, unconstrained iteration\n",
    "        stops\n",
    "    hist : bool\n",
    "        If True, returns array with value of x at every iteration. If False,\n",
    "        just returns last x value.o\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    2D or 3D numpy column array of floats\n",
    "        If hist is False, returns 2D numpy colmn array containing minimizing\n",
    "        x of problem. Otherwise returns a 3D numpy array containing every\n",
    "        value of x along the way.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_hist = []\n",
    "\n",
    "    n_e = p.num_eq_const()\n",
    "    n_i = p.num_ineq_const()\n",
    "    n_c = n_e + n_i\n",
    "\n",
    "    def W(x, lmb):\n",
    "        lmb_e = lmb[0:n_e, :]\n",
    "        lmb_i = lmb[n_e:n_c, :]\n",
    "        hess_f = _fd_hessian(p.cost, x)\n",
    "        hess_c_e = - np.sum([lmb_e[i] * _fd_hessian(p.eq_const()[i], x)\n",
    "            for i in range(0, n_e)])\n",
    "        hess_c_i = - np.sum([lmb_i[i] * _fd_hessian(p.ineq_const()[i], x)\n",
    "            for i in range(0, n_i)])\n",
    "        hess = hess_f + hess_c_e + hess_c_i\n",
    "        return hess\n",
    "\n",
    "    def A(x):\n",
    "        grad_e = np.array([np.squeeze(_fd_grad(p.eq_const()[i], x))\n",
    "                for i in range (0, n_e)])\n",
    "        grad_i = np.array([np.squeeze(_fd_grad(p.ineq_const()[i], x))\n",
    "                for i in range (0, n_i)])\n",
    "        if n_e != 0 and n_i != 0:\n",
    "            grad = np.concatenate((grad_e, grad_i), axis=0)\n",
    "        elif n_e != 0:\n",
    "            grad = grad_e\n",
    "        elif n_i != 0:\n",
    "            grad = grad_i\n",
    "        return grad\n",
    "\n",
    "    x = x0\n",
    "    lmb = np.zeros((n_c, 1))\n",
    "\n",
    "    # Concatenate costs\n",
    "    c_e = p.eq_const(x)\n",
    "    c_i = p.ineq_const(x)\n",
    "    if c_e is not None and c_i is not None:\n",
    "        c = np.concatenate((c_e, c_i), axis=0)\n",
    "    elif c_e is not None:\n",
    "        c = c_e\n",
    "    elif c_i is not None:\n",
    "        c = c_i\n",
    "\n",
    "    delta_x = 1e12\n",
    "\n",
    "    while delta_x  > tol:\n",
    "\n",
    "        # Compute KKT matrix\n",
    "        KKT = np.block([\n",
    "            [W(x, lmb), -A(x).T],\n",
    "            [-A(x), np.zeros((n_c, n_c))]\n",
    "        ])\n",
    "\n",
    "        # Compute gradient augmented with constraints\n",
    "        if n_e != 0 and n_i != 0:\n",
    "            f = np.block([\n",
    "                [-_fd_grad(p.cost, x).T + A(x).T @ lmb],\n",
    "                [p.eq_const(x)],\n",
    "                [p.ineq_const(x)]\n",
    "            ])\n",
    "        elif n_e != 0:\n",
    "            f = np.block([\n",
    "                [-_fd_grad(p.cost, x).T + A(x).T @ lmb],\n",
    "                [p.eq_const(x)]\n",
    "            ])\n",
    "        elif n_i != 0:\n",
    "            f = np.block([\n",
    "                [-_fd_grad(p.cost, x).T + A(x).T @ lmb],\n",
    "                [p.ineq_const(x)]\n",
    "            ])\n",
    "\n",
    "        x_prv = x\n",
    "        # Invert KKT matrix to get x and lambda increments\n",
    "        X = np.linalg.solve(KKT, f)\n",
    "        dim = np.max(np.shape(x))\n",
    "        x_hist.append(x)\n",
    "        # Apply x and lambda increments\n",
    "        x = x + X[:dim, :]\n",
    "        lmb = lmb + X[dim:, :]\n",
    "\n",
    "        c_e = p.eq_const(x)\n",
    "        c_i = p.ineq_const(x)\n",
    "\n",
    "        if c_e is not None and c_i is not None:\n",
    "            c = np.concatenate((c_e, c_i), axis=0)\n",
    "        elif c_e is not None:\n",
    "            c = c_e\n",
    "        elif c_i is not None:\n",
    "            c = c_i\n",
    "\n",
    "        # Check distance from previous x\n",
    "        delta_x = np.linalg.norm(x - x_prv)\n",
    "\n",
    "    return x if not hist else np.array(x_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.61803399]\n",
      " [ 1.61803399]]\n",
      "2.6180339887501933\n"
     ]
    }
   ],
   "source": [
    "v = lambda x: -x[0, 0] * x[1, 0]\n",
    "h1 = lambda x: -x[0, 0] - x[1, 0]**2 + 1\n",
    "h2 = lambda x: x[0, 0] + x[1, 0]\n",
    "p = Problem(v, ineq_const=[h1, h2])\n",
    "\n",
    "\n",
    "def test_lag(p):\n",
    "    x0 = np.array([[1], [1]])\n",
    "    x = lagrange_newton(p, x0, tol=1e-4)\n",
    "    print(x)\n",
    "    print(v(x))\n",
    "\n",
    "test_lag(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,1) (1,4,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(v(x))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=16'>17</a>\u001b[0m test_lag(p)\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 6'\u001b[0m in \u001b[0;36mtest_lag\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_lag\u001b[39m(p):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=11'>12</a>\u001b[0m     x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m], [\u001b[39m1\u001b[39m]])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m lagrange_newton(p, x0, tol\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000010?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(v(x))\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 4'\u001b[0m in \u001b[0;36mlagrange_newton\u001b[0;34m(p, x0, tol, hist)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000008?line=100'>101</a>\u001b[0m x_hist\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000008?line=101'>102</a>\u001b[0m \u001b[39m# Apply x and lambda increments\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000008?line=102'>103</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m X[:dim, :]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000008?line=103'>104</a>\u001b[0m lmb \u001b[39m=\u001b[39m lmb \u001b[39m+\u001b[39m X[dim:, :]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000008?line=105'>106</a>\u001b[0m c_e \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39meq_const(x)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,1) (1,4,1) "
     ]
    }
   ],
   "source": [
    "def V_1(x):\n",
    "    return np.abs(x[0]-1) + np.abs(x[1]-2)\n",
    "\n",
    "def h1_1(x):\n",
    "    return x[0]-x[1]**2\n",
    "\n",
    "def h2_1(x):\n",
    "    return x[0]**2 + x[1]**2 - 1\n",
    "p = Problem(V_1, eq_const =[h2_1], ineq_const=[h1_1])\n",
    "\n",
    "def test_lag(p):\n",
    "    x0 = np.array([[1], [1]])\n",
    "    x = lagrange_newton(p, x0, tol=1e-4)\n",
    "    print(x)\n",
    "    print(v(x))\n",
    "\n",
    "test_lag(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999999, 0.99999999]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_fd_grad(h2, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e730323216fec8a00495a18d5e26a9184315a136cb5e6ee6f99d5a78808c1dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
